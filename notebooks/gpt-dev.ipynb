{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build GPT: from scratch, in code, spelled out.\n",
    "\n",
    "A development notebook for building a nanoGPT, following along with Andrej Karpathy's \"Let's build GPT: from scratch, in code, spelled out.\" tutorial on YouTube.\n",
    "\n",
    "Github: https://github.com/jt-thorpe/Lets-Build-GPT.git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tiny Shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of text: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing\n",
    "\n",
    "We need to process the dataset and get something workable:\n",
    "- so things such as getting the unique characters from the dataset; the ***vocabulary***\n",
    "- useful info; such as size of vocabulary\n",
    "- tokenise the vocab; there are many ways to do this\n",
    "  - character level (which we are doing)\n",
    "  - word level\n",
    "  - subword level; tikToken from OpenAI or SentencePiece from Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  65\n",
      "Vocabulary: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Gather the vocabulary; that is, the unique characters in the text\n",
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size: \", vocab_size)\n",
    "print(f\"Vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text:  [21, 58, 5, 57, 1, 44, 56, 43, 43, 1, 56, 43, 39, 50, 1, 43, 57, 58, 39, 58, 43, 8, 8, 8]\n",
      "Decoded text:  It's free real estate...\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from character to numerical representation\n",
    "char_to_index = {char: index for index, char in enumerate(vocab)}\n",
    "index_to_char = {index: char for index, char in enumerate(vocab)}\n",
    "\n",
    "encode = lambda text: [char_to_index[char] for char in text]  # encoder: takes a string, returns a list of integers\n",
    "decode = lambda enc_text: ''.join([index_to_char[index] for index in enc_text])  # decoder: takes a list of integers, returns a string\n",
    "\n",
    "print(\"Encoded text: \", encode(\"It's free real estate...\"))\n",
    "print(\"Decoded text: \", decode(encode(\"It's free real estate...\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Our selection to use a character to integer mapping here reduces the size of our vocabulary considerably, but increases the length of our output considerably, compared to say per-word or per-subword encodings, which would increase the size of our vocabulary but decrease the length of our encoding outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0])\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire text, store it in a tensor\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:250])  # The same 250 characters as before, but now encoded as integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test/Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training and validation set\n",
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]\n",
    "\n",
    "#TODO: Why did Andrej use 0.9 for the training set here, why so much, 90% seems quite high? or is it not? Find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a model, we will not be feeding all our data to the model in one go, this is way to computationally inefficient. We want to feed in chunks, or blocks, at a time and train on that.\n",
    "\n",
    "In addition, we want our model to be \"*used to* training on as small a context size (block_size) as 1, so in the future, even it only sees one character it \"*knows*\" how to make predictions on such a small context size, and in between, all the way up to a context of block_size. As such, we need to define our block size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our data block_size\n",
    "block_size = 8\n",
    "\n",
    "train_data[:block_size + 1]  # +1 ensures we actually have 8 training examples for our block_size of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When input is tensor([18]) then target is 47\n",
      " When input is tensor([18, 47]) then target is 56\n",
      " When input is tensor([18, 47, 56]) then target is 57\n",
      " When input is tensor([18, 47, 56, 57]) then target is 58\n",
      " When input is tensor([18, 47, 56, 57, 58]) then target is 1\n",
      " When input is tensor([18, 47, 56, 57, 58,  1]) then target is 15\n",
      " When input is tensor([18, 47, 56, 57, 58,  1, 15]) then target is 47\n",
      " When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) then target is 58\n"
     ]
    }
   ],
   "source": [
    "# An example to show why +1 is necessary\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\" When input is {context} then target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, when we pass a block to the model, it is not just learning what the predicted next token is when it sees that block, it learns **all** of the possible combinations (in the order of the block still), for that block.\n",
    "\n",
    "So as demonstrated above, if we do not have +1 then we would only have 7 training examples for the model, for our block_size. This would mean we are missing the chance to learn the next predicted token for when there are 8 characters, hence we need an additional character (token).\n",
    "\n",
    "At each index, the i+1'th index is the target token the model will calculate a probability of appearing for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_size\n",
    "\n",
    "So we also need to define a batch_size, as when we are training our models on a GPU (which are designed for parallel processing), we want to be training lots of batches of chunks, all in parallel to each other as this is most efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "Targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      "-----\n",
      "\n",
      "When input is tensor([24]) then target is 43\n",
      "When input is tensor([24, 43]) then target is 58\n",
      "When input is tensor([24, 43, 58]) then target is 5\n",
      "When input is tensor([24, 43, 58,  5]) then target is 57\n",
      "When input is tensor([24, 43, 58,  5, 57]) then target is 1\n",
      "When input is tensor([24, 43, 58,  5, 57,  1]) then target is 46\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46]) then target is 43\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) then target is 39\n",
      "When input is tensor([44]) then target is 53\n",
      "When input is tensor([44, 53]) then target is 56\n",
      "When input is tensor([44, 53, 56]) then target is 1\n",
      "When input is tensor([44, 53, 56,  1]) then target is 58\n",
      "When input is tensor([44, 53, 56,  1, 58]) then target is 46\n",
      "When input is tensor([44, 53, 56,  1, 58, 46]) then target is 39\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39]) then target is 58\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) then target is 1\n",
      "When input is tensor([52]) then target is 58\n",
      "When input is tensor([52, 58]) then target is 1\n",
      "When input is tensor([52, 58,  1]) then target is 58\n",
      "When input is tensor([52, 58,  1, 58]) then target is 46\n",
      "When input is tensor([52, 58,  1, 58, 46]) then target is 39\n",
      "When input is tensor([52, 58,  1, 58, 46, 39]) then target is 58\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58]) then target is 1\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) then target is 46\n",
      "When input is tensor([25]) then target is 17\n",
      "When input is tensor([25, 17]) then target is 27\n",
      "When input is tensor([25, 17, 27]) then target is 10\n",
      "When input is tensor([25, 17, 27, 10]) then target is 0\n",
      "When input is tensor([25, 17, 27, 10,  0]) then target is 21\n",
      "When input is tensor([25, 17, 27, 10,  0, 21]) then target is 1\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1]) then target is 54\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) then target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # Set seed for reproducibility\n",
    "\n",
    "batch_size = 4  # How many independent sequences to process in parallel\n",
    "block_size = 8  # The length of each sequence (i.e. the context size for prediction)\n",
    "\n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # Starting index for each sequence\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])  # The input sequences\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # The target sequences\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Inputs: \")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"\\nTargets: \")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"\\n-----\\n\")\n",
    "\n",
    "for b in range(batch_size):  # batch dimension\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context} then target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x = torch.stack([data[i:i+block_size] for i in ix])  # The input sequences`\n",
    "\n",
    "`y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # The target sequences`\n",
    "\n",
    "By using the torch.stack here we are taking each 1d-tensor (e.g. [24, 43, 58,  5, 57,  1, 46, 43] etc...) and stack them as rows into one 4x8-d tensor, as shown in the demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # Set seed for reproducibility\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # A\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"Forward phase of the network.\"\"\"\n",
    "        # idx: (B, T) tensor of integers representing the indices of the context tokens\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (Batch = 4, Time = 8, Channels = vocab_size = 65)-d tensor\n",
    "\n",
    "        # This is to make our tensor conform to how cross_entropy expects the input\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # Stretch thr 1st and 2nd dimension into one, with C now being the 2nd dimension\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"Generate a sequence of new tokens.\n",
    "        \n",
    "        Works on the level of batches.\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities; a function to push values to 0-1 and sum to 1\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) because we want to sample one index\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the output above is the random gibberish we get from our model while it is not trained. Lets now train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the NNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimiser will adjust the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch optimiser\n",
    "optim = torch.optim.Adam(m.parameters(), lr=1e-3)  # Popular rates are: 3e-4 (on smaller networks, higher learning rates are more feasible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9999, loss: 2.3796486854553223\n"
     ]
    }
   ],
   "source": [
    "# A standard training loop\n",
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    # Backward pass\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "print(f\"Step {steps}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Remember, each time we run the model the gradients adjust (i.e. the parameters from the previous run are still here) each time we run it, improving our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "llo br. ave aviasurf my, mayo t ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;\n",
      "\n",
      "Whe, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty dedo bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "By bre ndy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n sar; my w, fredeeyong\n",
      "THek' merer, dd\n",
      "We ntem lud engitheso; cer ize helorowaginte the?\n",
      "Thak orblyoruldvicee chot, pannd e Yolde Th li\n"
     ]
    }
   ],
   "source": [
    "# Let's see what our model has learned\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can See, it is still gibberish, but this gibberish is starting to take form! (So fucking cool...)\n",
    "\n",
    "### So why is this model still producing nothing useful?\n",
    "\n",
    "Well, in this iteration the tokens that we are learning are not \"talking\" to each other, they aren't impacting each other. We're just looking at the alst character for each token when we're making a prediciton.\n",
    "\n",
    "We need to have these tokens look at each other within their context, to make better predictions. This leads us onto building a **transformer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Transformer\n",
    "\n",
    "### A mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the following toy example\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4,8,2 # Batch, Time, Channels - The batches, the time steps and the channels (i.e. some information at each point in the sequence)\n",
    "x = torch.randn(B, T, C) # Random input\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 8 token in a batch, and currently they do not talk to each other. We want to \"couple\" them.\n",
    "\n",
    "In partiuclar, in a very specific way. For example, the token in the 5th position should not couple with tokens in the 6,7,8th position. It should **not** couple with future tokens- only those in the past- so the 1,2,3,4th tokens.\n",
    "\n",
    "We cannot get any new information from the future, as we are about to try and *predict* the future. I.e. we're trying to predict the 6th token, using the info of the previous tokens.\n",
    "\n",
    "So how does a token communicate with the previous tokens. The simplest way would be to average the preceeding elements.\n",
    "\n",
    "So if I am the 5th token, I want to take the channels that are information at my step, but also the channels for the 4,3,2,1st steps also. I average these up and get a feature vector that summarises me (the 5th token) in the context of my history.\n",
    "\n",
    "This a ***very weak form of interaction***- this interaction is very lossy- we lose a ton of information about the spacial arrangements of those tokens, but this is okay for now.\n",
    "\n",
    "So what do we want to do for now:\n",
    "- for every single batch element independently (every t'th token)\n",
    "- calculate the average of all the vectors of the previous steps plus this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want `x[b,t] = mean_{i<=t} x[b,i]`\n",
    "xbow = torch.zeros((B, T, C))  # bow = bag of words; a common phrase when averaging over words\n",
    "\n",
    "# Loop not efficient, but good for understanding\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]  # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)  # Average over the time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So 1st row of xbow is an average of the 1st row of x\n",
    "- the 2nd row of xbow is an average of the 1st and 2nd row of x\n",
    "- 3rd row of xbow is an average of the 1st, 2nd and 3rd row of x\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here comes the trick!\n",
    "\n",
    "For efficiency, we dont need to loop, we can do this with matrix-multi very efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# Another toy example\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c=\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with some matrix magic, we can take a lower-triangle matrix of 1s and 0s (i.e. 0s in all i,j index above mid diag)\n",
    "\n",
    "So when we now do he dot product, we're basicially summing the rows as described above in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# Another toy example\n",
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c=\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! We now have efficient summing of rows without any looping, using matrix magic. Bon ap. With a small adjustment, we can then make this into the averages we desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# Another toy example\n",
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)  # Normalise the rows\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c=\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that c is now rows of averages of each row plus its previous.\n",
    "\n",
    "### Vectoring the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the following toy example\n",
    "torch.manual_seed(42)\n",
    "B, T, C = 4,8,2 # Batch, Time, Channels - The batches, the time steps and the channels (i.e. some information at each point in the sequence)\n",
    "x = torch.randn(B, T, C) # Random input\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Version 1\"\"\"\n",
    "# We want `x[b,t] = mean_{i<=t} x[b,i]`\n",
    "xbow = torch.zeros((B, T, C))  # bow = bag of words; a common phrase when averaging over words\n",
    "\n",
    "# Loop not efficient, but good for understanding\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]  # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)  # Average over the time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Version 2\"\"\"\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1, keepdim=True)  # Normalise the rows\n",
    "xbow2 = wei @ x  # (B,T,T) @ (B,T,C) ----> (B,T,C) (PyTorch will broadcast (add) the first dimension)\n",
    "\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Version 3\"\"\"\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # Set the upper triangle to -inf : \"The future cannot communicate with the past\"\n",
    "wei = F.softmax(wei, dim=1)  # Softmax over the time dimension\n",
    "xbow3 = wei @ x\n",
    "\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.9269,  1.4873],\n",
       "         [ 1.4138, -0.3091],\n",
       "         [ 1.1687, -0.6176],\n",
       "         [ 0.8657, -0.8644],\n",
       "         [ 0.5422, -0.3617],\n",
       "         [ 0.3864, -0.5354],\n",
       "         [ 0.2272, -0.5388],\n",
       "         [ 0.1027, -0.3762]]),\n",
       " tensor([[ 1.9269,  1.4873],\n",
       "         [ 1.4138, -0.3091],\n",
       "         [ 1.1687, -0.6176],\n",
       "         [ 0.8657, -0.8644],\n",
       "         [ 0.5422, -0.3617],\n",
       "         [ 0.3864, -0.5354],\n",
       "         [ 0.2272, -0.5388],\n",
       "         [ 0.1027, -0.3762]]),\n",
       " tensor([[ 1.9269,  1.4873],\n",
       "         [ 1.4138, -0.3091],\n",
       "         [ 1.1687, -0.6176],\n",
       "         [ 0.8657, -0.8644],\n",
       "         [ 0.5422, -0.3617],\n",
       "         [ 0.3864, -0.5354],\n",
       "         [ 0.2272, -0.5388],\n",
       "         [ 0.1027, -0.3762]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0], xbow3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Version 4: Self-attention!\"\"\"\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4,8,32  # Batch, Time, Channel\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# We create a \"head\" of self-attention\n",
    "head_size = 16  # A hyper-parameter\n",
    "key = nn.Linear(C, head_size, bias=False)  # Linear Function(?)\n",
    "query = nn.Linear(C, head_size, bias=False)  # Linear Function (?)\n",
    "value = nn.Linear(C, head_size, bias=False)  # \"I, token x, have a private internal value, if another token finds me intersting, this value is what I'll show you rather than internal value\"\n",
    "k = key(x)  # For x (me), this is what I have (B,T,16)-d\n",
    "q = query(x)  # For x (me), this is what I'm looking for (B,T,16)-d\n",
    "\n",
    "wei = q @ k.transpose(-2,-1)  # Transpose last 2 dimensions, not the batches: (B,T,16) @ (B,16,T) ---> (B,T,T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)  # We dont aggregate the raw x\n",
    "out = wei @ v\n",
    "\n",
    "out.shape\n",
    "\n",
    "#TODO: Tidy this up into code/markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]\n",
    "\n",
    "# Elements of the array with a higher value, are more interesting to that node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Story so far\n",
    "\n",
    "- So each node (token) `x` has calculated its own key and value\n",
    "  - key: \"this is what I am; the data I have\"\n",
    "  - query: \"this is what I am looking for\"\n",
    "  - e.g. \"I am a vowel in the 8th pos, looking for constanents, up-to the 4th position\"\n",
    "- For each node (token), we then forward these keys and queries in a linear function and produce k,v\n",
    "  - investigate this further - what is this forwarding?\n",
    "- To communicate, we then need to produce **data dependent** weights (affinities) between tokens\n",
    "- so take each token `x`, and compute the `queries @ keys`\n",
    "- We then need to mask the matrix, so that we dont check for future information\n",
    "-fill the weight matrix with -inf for all **future** tokens- we dont want to consider any values from the future\n",
    "- the weights in the wei matrix can be thought of each tokens affinity for another token- they are data dependent- some tokens will become interested in other tokens, and to varying amounts\n",
    "  - **When `q @ k.T` is high - i.e. the dot product is high, the two tokens have high affinity and vice-versa**\n",
    "- The weights then tell us essentially how much each token should aggregate the values from other tokens.\n",
    "- The softmax function takes out weights and normalises them between 0 and 1 (i.e. probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes from Karpathy\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See head.py for work so far - we've taken a single head of self-attention and implemeented this as multi-head in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer-Model-Architecture and what we have so far\n",
    "\n",
    "Ref: https://arxiv.org/abs/1706.03762v7\n",
    "\n",
    "Img from above article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![transformer-model-architecture](resources/transformer-model-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the bottom up, starting from the right-hand side:\n",
    "- We have our `Output Embedding` and ` Positional Encoding`\n",
    "  - this is our encoding of the vocabulary to integers\n",
    "- We have a `Maksed Multi-Head of Attention` which is our single head of attention implemented in parallel\n",
    "  - `masked` here refers to the masking of the weights vector that we have\n",
    "  - i.e. the lower-triangle matrix and softmax etc\n",
    "- ***We do not have the `Multi-Head of Attention` which is a cross-attention***\n",
    "- We will now look at the `Feed Forward` and come back to the cross-attention heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward\n",
    "\n",
    "FeedForward adds some computation into the model on a per-node (token) level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See FeedForward in head.py**\n",
    "\n",
    "- We define the FeedForward (A multi-layer perceptron)\n",
    "- We then call FeedForward sequentially in the Bigram model after we have self attended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what next?\n",
    "\n",
    "- We need to now intersperse the self-attention with the feed-forward neural network\n",
    "- i.e. we need to mix in the computation (the consideration of what each node has seen) into the network\n",
    "\n",
    "So what we want to do is, considering the fig from the paper, we want to implement the block. The section in grey labelled Nx (remembering we dont have the cross-attention yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block (Nx)\n",
    "\n",
    "See class Block in head.py\n",
    "\n",
    "Essentially just tying together what we have done so far, so that we can have multiple blocks of self-attention and computation going on.\n",
    "\n",
    "At this point however we begin to run into a problem, the neural network is starting to get **deep** and so we're starting to run into optimisation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimising our blocks\n",
    "\n",
    "![residual-connections](resources/highlighted-residual-connections.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 optimisiations we can make in regards to depending neural nets:\n",
    "- Residual (or skip) connections; highlighted above in red\n",
    "- and the Norm part of `Add & Norm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Connections\n",
    "\n",
    "First, lets look at Residual Connections.\n",
    "\n",
    "So we have a residual pathway, and if we want we are free to fork away from this pathway and do some computation in the attentional heads, and the project back (\"join\") back to the residual pathway via addition. Addition evenly distributes gradients throughout the the branches that were fed as input. So when back propping, the gradients from the loss go directly all the way back to the inputs through the addition nodes, and also fork off into the residual blocks.\n",
    "\n",
    "There is a \"gradient super highway\" all the way from the supervision, to the input, unimpeded. In the beginning the computation blocks are initialised to that they do not contribute much in the beginning and over time begin to contribute more and more to the residual pathway. This dramatically helps with efficiency.\n",
    "\n",
    "See: https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PPSV @ 1hr 30min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layernorm (Layer Normalisation)\n",
    "\n",
    "From the paper: https://arxiv.org/abs/1607.06450\n",
    "\n",
    "Layernorm is implemented in PyTorch. Essentially, we normalise over the layers (rows).\n",
    "\n",
    "See class LayerNorm in laynorm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Check out Karpathy Batch Normalisation video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now almost have a working model (minus the cross-attention module), and can look to scale this up. To scale up, we would look to do the following:\n",
    "- Increase the number of blocks that we're using; specified by the variable n_layer\n",
    "- Add in dropout (implemented in PyTorch); which essentially prevents a random subset of neurons in each pass from communicating, to help reduce overfitting\n",
    "  - https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "- Dropout is a regularisation technique"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-venv",
   "language": "python",
   "name": "nlp-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
